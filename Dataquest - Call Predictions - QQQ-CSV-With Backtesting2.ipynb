{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817babd2",
   "metadata": {},
   "source": [
    "# Stock Price Prediction\n",
    "Let's say we want to make money by buying stocks. Since we want to make money, we only want to buy stock on days when the price will go up (we're against shorting the stock). We'll create a machine learning algorithm to predict if the stock price will increase tomorrow. If the algorithm says that the price will increase, we'll buy stock. If the algorithm says that the price will go down, we won't do anything.\n",
    "\n",
    "We want to maximize our true positives - days when the algorithm predicts that the price will go up, and it actually goes go up. Therefore, we'll be using precision as our error metric for our algorithm, which is true positives / (false positives + true positives). This will ensure that we minimize how much money we lose with false positives (days when we buy the stock, but the price actually goes down).\n",
    "\n",
    "This means that we will have to accept a lot of false negatives - days when we predict that the price will go down, but it actually goes up. This is okay, since we'd rather minimize our potential losses than maximize our potential gains.\n",
    "\n",
    "## Method\n",
    "\n",
    "Before we get to the machine learning, we need to do a lot of work to acquire and clean up the data. Here are the steps we'll follow:\n",
    "\n",
    "- Download historical stock prices from Yahoo finance\n",
    "- Explore the data\n",
    "- Setup the dataset to predict future prices using historical prices\n",
    "- Test a machine learning model\n",
    "- Setup a backtesting engine\n",
    "- Improve the accuracy of the model\n",
    "\n",
    "At the end, we'll document some potential future directions we can go in to improve the technique.\n",
    "\n",
    "Downloading the data\n",
    "First, we'll download the data from Yahoo Finance. We'll save the data after we download it, so we don't have to re-download it every time (this could cause our IP to get blocked).\n",
    "\n",
    "We'll use data for a single stock (Microsoft) from when it started trading to the present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d91f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "msft_hist = pd.read_csv('qqq_data.csv', index_col=\"Date\", parse_dates=True)\n",
    "\n",
    "print (msft_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ccac4",
   "metadata": {},
   "source": [
    "As we can see, we have one row of data for each day that Microsoft stock was traded. Here are the columns:\n",
    "\n",
    "- Open - the price the stock opened at.\n",
    "- High - the highest price during the day\n",
    "- Low - the lowest price during the day\n",
    "- Close - the closing price on the trading day\n",
    "- Volume - how many shares were traded\n",
    "\n",
    "Stock doesn't trade every day (there is no trading on weekends and holidays), so some dates are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f3634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display microsoft stock price history so we can look at the structure of the data\n",
    "msft_hist.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45798f",
   "metadata": {},
   "source": [
    "Next, we'll plot the data so we can see how the stock price has changed over time. This gives us another overview of the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize microsoft stock prices\n",
    "msft_hist.plot.line(y=\"Close\", use_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab4156",
   "metadata": {},
   "outputs": [],
   "source": [
    "msft_hist.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908851e2",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Ok, hopefully you've stopped kicking yourself for not buying Microsoft stock at any point in the past 30 years now.\n",
    "\n",
    "Now, let's prepare the data so we can make predictions. We'll be predicting if the price will go up or down tomorrow based on data from today.\n",
    "\n",
    "First, we'll identify a target that we're trying to predict. Our target will be if the price will go up or down tomorrow. If the price went up, the target will be 1 and if it went down, the target will be 0.\n",
    "\n",
    "Next, we'll shift the data from previous days \"forward\" one day, so we can use it to predict the target price. This ensures that we don't accidentally use data from the same day to make predictions! (a very common mistake)\n",
    "\n",
    "Then, we'll combine both so we have our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ab03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we know the actual closing price\n",
    "data = msft_hist[[\"Close\"]]\n",
    "data = data.rename(columns = {'Close':'Actual_Close'})\n",
    "\n",
    "# Setup our target.  This identifies if the price went up or down\n",
    "data[\"Target\"] = msft_hist.rolling(2).apply(lambda x: x.iloc[1] > x.iloc[0])[\"Close\"]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c548692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift stock prices forward one day, so we're predicting tomorrow's stock prices from today's prices.\n",
    "msft_prev = msft_hist.copy()\n",
    "msft_prev = msft_prev.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dcacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "msft_prev.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c9358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our training data\n",
    "predictors = [\"Close\", \"Open\", \"High\", \"Low\", \"Upper\", \"Lower\", \"EMA10\", 'H/L%', \"Sentiment Avg\", \"Smoothing Line10\", \"EMA40\", \"Smoothing Line40\", \"Smoothing Line\", \"Volume\", \"Volume MA\", \"RSI\", \"RSI-based MA\", \"ROC\", \"Williams\", \"Histogram\", \"MACD\", \"Signal\", \"CCI\", \"Disparity Index\", \"SES\"]\n",
    "data = data.join(msft_prev[predictors]).iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d5968",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd3bd30",
   "metadata": {},
   "source": [
    "## Creating a machine learning model\n",
    "Next, we'll create a machine learning model to see how accurately we can predict the stock price.\n",
    "\n",
    "Because we're dealing with time series data, we can't just use cross-validation to create predictions for the whole dataset. This will cause leakage where data from the future will be used to predict past prices. This doesn't match with the real world, and will make us think that our algorithm is much better than it actually is.\n",
    "\n",
    "Instead, we'll split the data sequentially. We'll start off by predicting just the last 100 rows using the other rows.\n",
    "\n",
    "We'll use a random forest classifier to generate our predictions. This is a good \"default\" model for a lot of applications, because it can pick up nonlinear relationships in the data, and is somewhat robust to overfitting with the right parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e5dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Create a random forest classification model.  Set min_samples_split high to ensure we don't overfit.\n",
    "model = RandomForestClassifier(n_estimators=2378, min_samples_split=200, random_state=1)\n",
    "\n",
    "# Create a train and test set\n",
    "train = data.iloc[:-150]\n",
    "test = data.iloc[-150:]\n",
    "\n",
    "model.fit(train[predictors], train[\"Target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4276860",
   "metadata": {},
   "source": [
    "Next, we'll need to check how accurate the model was. Earlier, we mentioned using precision to measure error. We can do this by using the precision_score function from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096295d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "\n",
    "# Evaluate error of predictions\n",
    "preds = model.predict(test[predictors])\n",
    "preds = pd.Series(preds, index=test.index)\n",
    "precision_score(test[\"Target\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1f024",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.tail(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fbd2d2",
   "metadata": {},
   "source": [
    "So our model is directionally accurate XX% of the time. This is only a little bit better than a coin flip! We can take a deeper look at the individual predictions and the actuals, and see where we're off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f41d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat({\"Target\": test[\"Target\"],\"Predictions\": preds}, axis=1)\n",
    "combined.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e50e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test[\"Target\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b8806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "information = [test[\"Target\"], preds]\n",
    "pd.DataFrame(information).transpose().to_csv('info-test2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a236073",
   "metadata": {},
   "source": [
    "## Backtesting\n",
    "Our model isn't great, but luckily we can still improve it. Before we do that, let's figure out how to make predictions across the entire dataset, not just the last 100 rows. This will give us a more robust error estimate. The last 100 days may have has atypical market conditions or other issues that make error metrics on those days unrealistic for future predictions (which are what we really care about).\n",
    "\n",
    "To do this, we'll need to backtest. Backtesting ensures that we only use data from before the day that we're predicting. If we use data from after the day we're predicting, the algorithm is unrealistic (in the real world, you won't be able to use future data to predict that past!).\n",
    "\n",
    "Our backtesting method will loop over the dataset, and train a model every 750 rows. We'll make it a function so we can avoid rewriting the code if we want to backtest again.\n",
    "\n",
    "In the backtesting function, we will:\n",
    "\n",
    "- Split the training and test data\n",
    "- Train a model\n",
    "- Make predictions on the test data using predict_proba - this is because we want to really optimize for true positives. By default, the threshold for splitting 0/1 is .5, but we can set it to different values to tweak the precision. If we set it too high, we'll make fewer trades, but will have a lower potential for losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99665972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(data, model, predictors, start=1000, step=10):\n",
    "    predictions = []\n",
    "    # Loop over the dataset in increments\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        # Split into train and test sets\n",
    "        train = data.iloc[0:i].copy()\n",
    "        test = data.iloc[i:(i+step)].copy()\n",
    "        \n",
    "        # Fit the random forest model\n",
    "        model.fit(train[predictors], train[\"Target\"])\n",
    "        \n",
    "        # Make predictions\n",
    "        preds = model.predict_proba(test[predictors])[:,1]\n",
    "        preds = pd.Series(preds, index=test.index)\n",
    "        preds[preds > .5] = 1\n",
    "        preds[preds<=.5] = 0\n",
    "        \n",
    "        # Combine predictions and test values\n",
    "        combined = pd.concat({\"Target\": test[\"Target\"],\"Predictions\": preds}, axis=1)\n",
    "        \n",
    "        predictions.append(combined)\n",
    "    \n",
    "    return pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b623d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = backtest(data, model, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fadd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"Predictions\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216a755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(predictions[\"Target\"], predictions[\"Predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fade61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(predictions[\"Target\"], predictions[\"Predictions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d101a",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb5c4f",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "We've come far in this project! We've downloaded and cleaned data, and setup a backtesting engine. We now have an algorithm that we can add more predictors to and continue to improve the accuracy of.\n",
    "\n",
    "There are a lot of next steps we could take to improve our predictions:\n",
    "\n",
    "## Improve the technique\n",
    "- Calculate how much money you'd make if you traded with this algorithm\n",
    "\n",
    "## Improve the algorithm\n",
    "- Run with a reduced step size! This will take longer, but increase accuracy\n",
    "- Try discarding older data (only keeping data in a certain window)\n",
    "- Try a different machine learning algorithm\n",
    "- Tweak random forest parameters, or the prediction threshold\n",
    "\n",
    "## Add in more predictors\n",
    "- Account for activity post-close and pre-open.\n",
    "  - Early trading\n",
    "  - Trading on other exchanges that open before the NYSE (to see what the global sentiment is)\n",
    "- Economic indicators\n",
    "  - Interest rates\n",
    "  - Other important economic news\n",
    "- Key dates\n",
    "  - Dividends\n",
    "  - External factors like elections\n",
    "- Company milestones\n",
    "  - Earnings calls\n",
    "  - Analyst ratings\n",
    "  - Major announcements\n",
    "- Prices of related stocks\n",
    "  - Other companies in the same sector\n",
    "  - Key partners, customers, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
